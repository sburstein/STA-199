---
title: "Exam 02"
author: "Scott Burstein"
subtitle: STA 199, Fall 2020, Duke University
output:
  pdf_document: default
  html_document: default
---

# Setup

```{r eval = FALSE}
library(usethis)
use_git_config(user.name= "sburstein", user.email="scott.burstein@duke.edu")
```

```{r load-packages, message=FALSE, warning=FALSE}
library(tidyverse) 
``` 

```{r read-in-data, message = FALSE}
mn_homes <- read_csv("data/mn_homes.csv")
colnames(mn_homes)
```

Remember to configure Git, include your name in the "author" section of the 
header, label all code chunks, and commit and push regularly!

# Exercise 1

Create new var `pricesqft`: price per square foot of each house
```{r pricesqft}
mn_homes <- mn_homes %>% 
  mutate(pricesqft = round(salesprice / area, 2))
```

Create new var `fireplace`: an indicator for the presence of a fireplace
```{r fireplace}
mn_homes <- mn_homes %>% 
  mutate(fireplace = ifelse(numfireplaces > 0, 1, 0))
```

Create new var `holidays`: an indicator for whether the home was sold in 
Oct, Nov, or Dec
```{r holidays}
mn_homes <- mn_homes %>% 
  mutate(holidays = ifelse(salemonth > 9, 1, 0))
```

Create new var `prewar`: an indicator for whether the home was built during or 
before 1940
```{r prewar}
mn_homes <- mn_homes %>% 
  mutate(prewar = ifelse(yearbuilt <= 1940, 1, 0))
```

```{r view-dataframe}
view(mn_homes)
```

# Exercise 2

Create dataset `min_max_neighborhood` containing the houses with the 
smallest and largest price per square foot in each neighborhood.
```{r min-max-ppsf-per-neighborhood}

# dataframe with min ppsf values:
min_neighborhood <- mn_homes %>% 
  group_by(neighborhood) %>% 
  filter(pricesqft == min(pricesqft)) %>% 
  select(salesprice, neighborhood, pricesqft)

# dataframe with max ppsf values:
max_neighborhood <- mn_homes %>% 
  group_by(neighborhood) %>% 
  filter(pricesqft == max(pricesqft)) %>% 
  select(salesprice, neighborhood, pricesqft)

# combine 2 dataframes:
min_max_neighborhood <- rbind(min_neighborhood, max_neighborhood)

# view Minnehaha values:
min_max_neighborhood %>% 
  filter(neighborhood == "Minnehaha") %>% 
  print()
```

# Exercise 3

Create a new dataset `month_count` containing the count of homes sold in each 
month across all years.

```{r sales_by_month_count}
month_count <- mn_homes %>% 
  group_by(salemonth) %>% 
  mutate(monthcount = n()) %>% 
  select(salemonth, monthcount) %>% 
  distinct() %>% 
  arrange(salemonth)
month_count
```

visualization of the count of homes sold by month:
```{r month_count_visualization}
month_count_viz <- month_count %>% 
  ggplot(aes(x = salemonth, y = monthcount)) +
  geom_bar(stat = "identity") +
  labs(x = "Sale Month",
       y = "Number of Homes Sold",
       title = "Count of Homes Sold by Month") +
  scale_x_continuous(breaks=seq(1,12,1))+
  theme_minimal()
month_count_viz
```

More homes are sold in the summer months than in other seasons. Based on the 
simple bar plot, it is evident that the number of homes sold per month peaks in 
June, when ~647 homes are sold on average. 
There is a clear unimodal distribution of the number of homes sold per month, centered at June and the summer months. 
The winter months have the fewest homes sold per month on average.

# Exercise 4

Sample correlation between lot size and area:
```{r lotsize_area_correlation}

# Correlation Coefficient:
samp_corr <- cor(mn_homes$lotsize, mn_homes$area)
samp_corr

# Correlation Coefficient Test and Significance Level:
samp_corr_test <- cor.test(mn_homes$lotsize, mn_homes$area)
samp_corr_test

# r value can also be computed manually:
corr_lotsize_area <- mn_homes %>%
  summarize(r_value = cor(lotsize, area)) %>%
  print()
```

Construct a 99% bootstrap confidence interval for the correlation between 
lotsize and area:

```{r ex-4 correlation_bootstrap}
boot_dist <- numeric(1000)
for (i in 1 : 1000) {
  set.seed(i)
  indices <- sample(1 : nrow(mn_homes), replace = TRUE)
  boot_correlation <- mn_homes %>%
  slice(indices) %>%
  summarize(r_value = cor(lotsize, area)) %>%
    pull()
  boot_dist[i] <- boot_correlation
}
boot_cor <- tibble(boot_dist)
bootstrap_interval <- boot_cor %>%
  summarize(lower = quantile(boot_dist, 0.005),
            upper = quantile(boot_dist, 0.995)
            )
bootstrap_interval
```

Visualize the bootstrap distribution and confidence interval from
above:

```{r ex-4 corr-bootstrap-visualization}
ggplot(data = boot_cor, aes(x = boot_dist)) +
  geom_histogram(color = "black", fill = "blue", alpha = .8) + 
  geom_vline(xintercept = c(bootstrap_interval$lower, 
                            bootstrap_interval$upper),
             color = "red") +
  labs(title = "Bootstrap Distribution of Correlation",
       subtitle =  "Between MN Home Lot Size and Area",
       x = "Bootstrap Correlation Between Lot Size and Area",
       y = "Count") +
  theme_light()
```

If we compute a large number of samples (i.e. 1,000) of the given data, 
99% of those simulations would yield correlations within the marked interval (0.3883714, 0.484099) visualized as red vertical lines. 
Thus, we can be 99% confident that these bounds contain the true correlation between Minnesota home lot size and area.

# Exercise 5

A real estate agent working in the community Southwest is trying to convince 
individuals to purchase a home by supplying an interval estimate of the true 
mean lot size of sold homes. Larger lot sizes means more room for activities: 
grilling, playing catch, swingsets, etc.

Construct a 90% confidence interval for the mean lot size of houses sold in the 
Southwest community using both a simulation-based approach and a CLT-based 
approach.

**Filter for SW Homes Only**
Where `community` == "Southwest":
```{r sw-subset}
sw_mn_homes <- mn_homes %>% 
  filter(community == "Southwest")
```

**Simulation-Based Approach:**
Simulation Seed set at 22 (set.seed(22))
```{r ex-5 sim-based-mean}
set.seed(22)
n_sims <- 5000
stat <- numeric(n_sims)
for (i in 1:n_sims) {
  indices <- sample(1:nrow(sw_mn_homes), replace = TRUE)
  boot_mean <- sw_mn_homes %>%
    slice(indices) %>%
    summarize(boot_mean = mean(lotsize)) %>%
    pull()
  stat[i] <- boot_mean
}
boot_means <- tibble(stat = stat)
```

```{r ex-5 bootstrap_interval}
bootstrap_interval <- boot_means %>%
  summarize(lower = quantile(stat, 0.050),
            upper = quantile(stat, 0.950))
bootstrap_interval
```

The 90% confidence interval for the mean lot size in square feet is 
(6237.109, 6448.654) according to the bootstrap method.

From the simulation based approach, we can be 90% confident that the true 
average/mean lot size for sampled Minnesota homes in the community *Southwest* 
is between the bounds of 6,237.1 and 6,448.7 square feet 
(based on mean probabilities).

**CLT-based approach:**

```{r ex-5 CLT-mean-interval}
t.test(sw_mn_homes$lotsize, na.rm=TRUE, conf.level = 0.90)
```

The 90% confidence interval for the mean lot size in square feet is 
(6233.572, 6441.746) according to the Central Limit Theorem (CLT).

From the CLT based approach, we can be 90% confident that the true 
average/mean lot size for sampled Minnesota homes in the community *Southwest* 
is between the bounds of 6,233.6 and 6,441.7 square feet 
(based on mean probabilities).

# Exercise 6

**Mean Lot Size:**

```{r ex-6 lotsize_mean}
mean(sw_mn_homes$lotsize)
```

$\mu$ = 6337.659

For hypotheses $H_o: \mu = 10,134$ and $H_a: \mu \neq 10,134$,
a hypothesis test conducted using $\alpha = 0.10$ would 
provide sufficient evidence to reject the null hypothesis.

Looking at the 90% confidence interval from Exercise 5 (above), 
where alpha = 0.10 since:

$$Confidence~Level + \alpha = 1 $$
It is evident that neither the 90% confidence interval constructed using the 
bootstrap simulation-based approach (6237.109, 6448.654) 
nor the CLT-based approach (6233.572, 6441.746) 
contained a mean value of $10,134. 

Thus, a hypothesis test conducted using $\alpha = 0.10$ would 
reject the null hypothesis that $H_o: \mu = 10,134$, since this $H_o$ value is 
outside both 90% confidence intervals created in Exercise 5 using 
(1) a simulation-based and (2) a CLT-based approach.

# Exercise 7

**Background:**
Homes with fireplaces are cozy and inviting, particularly around winter 
holidays. Does the presence of a fireplace help convince a buyer to purchase a 
home when it is cold outside?

**Question:**
Does the presence of a fireplace help convince a buyer to purchase a home when 
it is cold outside?

**Directive:**
Comprehensively assess the claim that a higher proportion of homes sold during 
October, November, and December have fireplaces compared to homes sold during 
other months.

### Analysis Strategy:
I will take advantage of `holiday` boolean to determine whether MN home was sold 
during October, November, and December (`holiday` == 1) or not (`holiday` == 0),
as well as `fireplace` boolean to determine whether MN home sold has one or more 
fireplace(s) (`fireplace` == 1) or not (`fireplace` == 0).

```{r homes_sold_by_winter_and_fireplace}
winter_fireplace_sales <- mn_homes %>%
  group_by(holidays, fireplace) %>%
  summarise(sold_count = n()) %>%
  ungroup()
winter_fireplace_sales
```

***
### Hypotheses:

#### $H_o:$

Probability fireplace for holiday MN home sales $\leq$ Probability fireplace for 
non-holiday MN home sales.

The proportion of MN homes that have fireplaces, sold in October, November, or 
December months is less than or equal to the proportion of MN home sales that 
have fireplaces in non-holiday months.

***

#### $H_a:$

Probability fireplace for holiday MN home sales $\gt$ Probability fireplace for 
non-holiday MN home sales.

The proportion of MN homes that have fireplaces, sold in October, November, or 
December months is greater than the proportion of MN home sales that have 
fireplaces in non-holiday months.

***

### Summary Proportions:
```{r total-prop-fireplace}
total_props_fireplace <- mn_homes %>%
  summarise(prop_fireplace = sum(fireplace)/n())
total_props_fireplace
```

From the entire dataset, the proportion of MN home sales with fireplaces 
is 0.416.

```{r holidays-prop-fireplace}
holidays_props_fireplace <- mn_homes %>%
  filter(holidays == 1) %>% 
  summarise(prop_fireplace = sum(fireplace)/n())
holidays_props_fireplace
```

From the holidays MN home sales subset, the proportion of MN home sales with 
fireplaces is 0.4215168.

```{r non-holidays-prop-fireplace}
non_holidays_props_fireplace <- mn_homes %>%
  filter(holidays == 0) %>% 
  summarise(prop_fireplace = sum(fireplace)/n())
non_holidays_props_fireplace
```

From the non-holidays MN home sales subset, the proportion of MN home sales with 
fireplaces is 0.4143818.

***

### Re-iteration of Hypotheses:

***

#### If we fail to reject $H_o:$

To the necessary level of significance...

Proportion of `fireplace` == 1 for MN home sales where `holidays` == 1 

$\leq$ 

Proportion of `fireplace` == 1 MN for home sales where `holidays` == 0.

**AND**

Proportion of `fireplace` == 0 MN for home sales where `holidays` == 1

$\geq$ 

Proportion of `fireplace` == 0 MN for home sales where `holidays` == 0.

***

#### Conversely, if we have sufficient evidence to reject $H_o, then:$

Proportion of `fireplace` == 1 for MN home sales where `holidays` == 1

$\gt$ 

Proportion of `fireplace` == 1 for MN home sales where `holidays` == 0.

**AND**

Proportion of `fireplace` == 0 for MN home sales where `holidays` == 1

$\lt$ 

Proportion of `fireplace` == 0 for MN home sales where `holidays` == 0.

***

### Bootstrap Estimation:

```{r bootstrap_estimate}
set.seed(144)
number_sims <- 2500

holiday_sales <- mn_homes %>% 
  filter(holidays == 1)

other_sales <- mn_homes %>% 
  filter(holidays == 0)

simulated_props <- numeric(number_sims)

for (i in 1:number_sims) {
  
  h_prop <- holiday_sales %>%
  slice(sample(1:nrow(holiday_sales), replace = TRUE)) %>%
  summarize(sum(fireplace)/n()) %>%
  pull()
  
  o_prop <- other_sales %>%
  slice(sample(1:nrow(other_sales), replace = TRUE)) %>%
  summarize(sum(fireplace)/n()) %>%
  pull()
  
  simulated_props[i] <- h_prop - o_prop
}
```

***

### P-Value:

```{r difference_holidays}

holiday_diffs <- tibble(diffs = simulated_props)

boot_mean <- holiday_diffs %>%
  summarize(mean = mean(diffs)) %>% 
  pull()

holiday_diffs <- holiday_diffs %>%
  mutate(shifted_diffs = diffs + (0 - boot_mean))

holiday_diffs %>%
  filter(shifted_diffs > boot_mean) %>%
  summarize(p_value = n()/nrow(holiday_diffs))

```

***

Visualize the simulated null distribution and observed data on a single, effective visualization:

### Bootstrap Histogram:

```{r ex-7 vizualization}
#simulated_props_data <- tibble(props = simulated_props)
simulated_props_data <- holiday_diffs

#obs_mean <- mean(mn_homes$fireplace) #total prop w/ fireplace is 0.416
obs_diff <- 0.4215168 - 0.4143818
ggplot(simulated_props_data, 
    aes(x = shifted_diffs)) + 
  geom_histogram(bins = 30) + 
  geom_vline(xintercept = (obs_diff), color = "red") +
  labs(x = "Simulated Difference Between Holiday and non-Holiday Sales",
       title = "For Proportion of MN Homes Sold with Fireplace(s)")
```

```{r ex-7 p-value-holidays}
simulated_props_data %>%
  filter(shifted_diffs >= (obs_diff)) %>%
  summarize(p_value = n() / nrow(simulated_props_data))
```

The p-value for the difference in proportions between fireplaces in the holiday season sold homes and fireplaces in the non-holiday season sold homes is significantly greater than 0.05, which is insufficient evidence to reject the 
null hypothesis while using a 95% confidence interval.

In other words, since the p-value = 0.33, we fail to reject the null hypothesis 
that the proportion of MN homes that have fireplaces, sold in October, November, or December months is less than or equal to the proportion of MN home sales that 
have fireplaces in non-holiday months.

# Exercise 8

Pre-war homes (constructed during and before 1940) are known for their sturdy construction and charm, including elements like high ceilings, decorative moldings, and hardwood floors.

I will comprehensively assess the claim that pre-war homes have a higher median price per square foot compared to non pre-war homes, using a simulation-based method.

```{r ex-8}

set.seed(88)
n_sims <- 2500

pre_war <- mn_homes %>%
  filter(prewar == 1)

post_war <- mn_homes %>%
    filter(prewar == 0)



boot_sims <- numeric(n_sims)

for (i in 1:n_sims) {

  prewar_ppsf <- pre_war %>%
    slice(sample(1:nrow(pre_war), replace = TRUE)) %>%
    summarize(median(pricesqft)) %>%
    pull()

  postwar_ppsf <- post_war %>%
    slice(sample(1:nrow(post_war), replace = TRUE)) %>%
    summarize(median(pricesqft)) %>%
    pull()
  
  boot_sims[i] <- prewar_ppsf - postwar_ppsf

}

obs_diff <- median(pre_war$pricesqft) - median(post_war$pricesqft)

boot_median <- mean(boot_sims)

boot_sims <- tibble(boot_sims)
centered <- boot_sims %>%
  mutate(centered = boot_sims - boot_median)

centered %>%
  mutate(extreme = if_else(centered >= obs_diff, 1, 0)) %>%
  summarize(p_val = mean(extreme)) %>%
  pull()
```

Our null hypothesis is $M_{prewar}$ $\leq$ $M_{postwar}$ , 
where $M_{prewar}$ is the median price per square foot for pre war 
(built before 1940) homes in the dataset. 

$M_{postwar}$ is the median price per square foot for pre war 
(built before 1940) homes in the dataset. 

Our alternative hypothesis is $M_{prewar}$ $\gt$ $M_{postwar}$ . 

Our p-value was 0.0192, and so we reject the null hypothesis at the 
$\alpha$ = 0.05 level. 

There is sufficient evidence to suggest that the median price per 
square foot of homes built prewar (before 1940) is greater than the 
median price per square foot of homes built postwar (after 1940).

# Exercise 9

Suppose you are interested in testing the claim that more than 50% of 
Minneapolis residents own a pet. You take a random sample of 104 residents and find that 53.8% own a pet. Describe the procedure for constructing a simulation-based null distribution and finding the p-value.

First, I would determine what the null and alternative hypotheses are. In this 
case, the null hypothesis would be that less than or equal to 50% of Minneapolis 
residents own a pet. The alternative hypothesis would be that greater than 50% 
of Minneapolis residents own a pet.

Not referencing specific R code, I will use the red and white marble scheme as 
outlined in the problem statement for the following steps. Each marble is a 
resident. Red marbles correspond to a pet owner. White marbles correspond to 
residents without pets.

Second, I would fill an imaginary bucket with 104 marbles, 52 red and 52 white (0.5 * 104 = 52). 

The summary statistic given is the sample proportion that 53.8% (0.538) of Minneapolis residents own a pet.

Thus, I am looking to calculate the probability of getting data as or more 
extreme as p = 0.538 if the null hypothesis is true. This is the p-value.

To do so, I will sample (pull) 104 marbles from the imaginary bucket described 
above WITH REPLACEMENT. After each individual pull, I will record whether the marble was red or white, and replace the marble. At the end, I will determine 
how many red and white marbles were pulled in total, and calculate the 
proportion that were home owners (# red marbles / 104).

I will repeat this process of pulling 104 marbles with 
replacement a large number of times, say 1,000. 

At the end, I will sort all of these sampled red marble proportions from the 
1,000 trials in a table from smallest to largest.

Using a 95% confidence interval for a one-sided hypothesis test, I know that I 
am looking to see if 0.538 is in the first 95% of the sampled distribution 
(less than or equal to the 950th proportion in the sorted table). 

If it is, then there is insufficient evidence to reject the null hypothesis that less than or equal to 50% of Minneapolis residents own a pet.

Otherwise, if it is greater than the 950th proportion, then there is sufficient evidence to reject the null hypothesis, and we can likely conclude that that greater than 50% of Minneapolis residents own a pet.

# Exercise 10

A random sample of 1,231 Minneapolis residents is taken and 0.5207 report that they rent (rather than own) their home. A 95% confidence interval for the true proportion of Minneapolis residents who rent their homes is (0.492, 0.549).

Identify the following statements as TRUE or FALSE. No justification is 
necessary.

(a) There is a 95% chance the true population proportion is between 0.492 and 0.549.

TRUE

(b) If we considered many random samples of Minneapolis residents of size 1,231 and calculated 95% confidence intervals for each sample, approximately 95% of them will contain the true population proportion.

TRUE

(c) If we considered many random samples of Minneapolis residents of size 1,231 and calculated 95% confidence intervals for each sample, approximately 95% of the sample proportions will be between 0.492 and 0.549.

TRUE

(d) We are 95% confident the sample proportion is between 0.492 and 0.549.

TRUE

# Exercise 11

Identify the following statements as TRUE or FALSE. Provide a brief, one 
sentence justification for statements that are FALSE. No justification is necessary for TRUE statements.

(a) A p-value of 0.01 means the null hypothesis has a 1% chance of being true.

FALSE
A p-value of 0.01 means that assuming the null hypothesis is true, the 
probability of seeing results as or more extreme as the sample is 0.01.

(b) A nonsignificant difference (p-value > 0.05) means there is no difference between groups.

TRUE 
(no statistical difference)


(c) A p-value < 0.05 means we have a relationship of practical importance.

TRUE 
(p-value < 0.05 is sufficient to reject the null hypothesis)

(d) Smaller p-values imply the presence of larger or more important effects.

FALSE
p-values less than $\alpha$ provide statistically significant evidence to 
reject the null hypothesis. Comparing relative p-values is futile because it 
does not measure the size of relative effects or importance.

(e) We reject the null hypothesis if the p-value is less than $\alpha$.

TRUE
